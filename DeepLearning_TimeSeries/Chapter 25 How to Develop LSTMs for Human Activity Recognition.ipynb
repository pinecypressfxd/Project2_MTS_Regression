{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee8e750",
   "metadata": {},
   "source": [
    "# Chapter 25 How to Develop LSTMs for Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6daee0",
   "metadata": {},
   "source": [
    "In this tutorial, you will discover three recurrent neural network architectures for modeling an activity recognition time series classification problem. After completing this tutorial, you will know:\n",
    "- How to develop a Long Short-Term Memory Recurrent Neural Network for human activity recognition.\n",
    "- How to develop a one-dimensional Convolutional Neural Network LSTM, or CNN-LSTM, model.\n",
    "- How to develop a one-dimensional Convolutional LSTM, or ConvLSTM, model for the same problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411a68ea",
   "metadata": {},
   "source": [
    "## 25.1 Tutorial Overview\n",
    "This tutorial is divided into four parts; they are:\n",
    "1. Activity Recognition Using Smartphones Dataset \n",
    "2. LSTM Model\n",
    "3. CNN-LSTM Model\n",
    "4. ConvLSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b2ed2",
   "metadata": {},
   "source": [
    "## 25.2 Activity Recognition Using Smartphones Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65724ae",
   "metadata": {},
   "source": [
    "## 25.3 LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a56c13",
   "metadata": {},
   "source": [
    "For more information on LSTMs for time series forecasting, see Chapter 9. This section is divided into four parts; they are:\n",
    "1. Load Data\n",
    "2. Fit and Evaluate Model \n",
    "3. Summarize Results\n",
    "4. Complete Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4dc578",
   "metadata": {},
   "source": [
    "### 25.3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4e778f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c54c8",
   "metadata": {},
   "source": [
    "### 25.3.2 Fit and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574109c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58ce3c",
   "metadata": {},
   "source": [
    "### 25.3.3 Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29358398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores) \n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "    \n",
    "# run an experiment\n",
    "def run_experiment(repeats=10):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy) \n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score)) \n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa297a6",
   "metadata": {},
   "source": [
    "### 25.3.4 Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610c6fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 92.399\n",
      ">#2: 89.141\n",
      ">#3: 88.191\n",
      "[92.39904880523682, 89.1414999961853, 88.19137811660767]\n",
      "Accuracy: 89.911% (+/-1.802)\n"
     ]
    }
   ],
   "source": [
    "# lstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    verbose, epochs, batch_size = 0, 15, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1] \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(n_timesteps,n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores) \n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "    \n",
    "# run an experiment\n",
    "def run_experiment(repeats=3):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy) \n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score)) \n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run the experiment\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbe011",
   "metadata": {},
   "source": [
    "## 25.4 CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7317438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 89.447\n",
      ">#2: 91.517\n",
      ">#3: 90.635\n",
      "[89.44689631462097, 91.51679873466492, 90.63454270362854]\n",
      "Accuracy: 90.533% (+/-0.848)\n"
     ]
    }
   ],
   "source": [
    "# lstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_features, n_outputs = trainX.shape[2], trainy.shape[1]\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features)) \n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(64, 3, activation='relu'),\n",
    "    input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(64, 3, activation='relu'))) \n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D()))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores) \n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "    \n",
    "# run an experiment\n",
    "def run_experiment(repeats=3):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy) \n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score)) \n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run the experiment\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4e6f7",
   "metadata": {},
   "source": [
    "## 25.5 ConvLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28990181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">#1: 90.533\n",
      ">#2: 88.531\n",
      ">#3: 91.381\n",
      "[90.53274393081665, 88.53070735931396, 91.3810670375824]\n",
      "Accuracy: 90.148% (+/-1.195)\n"
     ]
    }
   ],
   "source": [
    "# convlstm model for the har dataset\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "    # zero-offset class values\n",
    "    trainy = trainy - 1\n",
    "    testy = testy - 1\n",
    "    # one hot encode y\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_features, n_outputs = trainX.shape[2], trainy.shape[1]\n",
    "    # reshape data into time steps of sub-sequences\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features)) \n",
    "    testX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(64, 3, activation='relu'),\n",
    "    input_shape=(None,n_length,n_features)))\n",
    "    model.add(TimeDistributed(Conv1D(64, 3, activation='relu'))) \n",
    "    model.add(TimeDistributed(Dropout(0.5)))\n",
    "    model.add(TimeDistributed(MaxPooling1D()))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# fit and evaluate a model\n",
    "def evaluate_model(trainX, trainy, testX, testy):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_features, n_outputs = trainX.shape[2], trainy.shape[1]\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "    n_steps, n_length = 4, 32\n",
    "    trainX = trainX.reshape((trainX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    testX = testX.reshape((testX.shape[0], n_steps, 1, n_length, n_features))\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(64, (1,3), activation='relu', input_shape=(n_steps, 1, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax')) \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    # fit network\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores) \n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "    \n",
    "# run an experiment\n",
    "def run_experiment(repeats=3):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score = evaluate_model(trainX, trainy, testX, testy) \n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score)) \n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # run the experiment\n",
    "    run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbb24f",
   "metadata": {},
   "source": [
    "As with the prior experiments, running the model prints the performance of the model each time it is fit and evaluated. A summary of the final model performance is presented at the end of the run. We can see that the model does consistently perform well on the problem achieving an accuracy of about 90%, perhaps with fewer resources than the larger CNN-LSTM model.\n",
    "\n",
    "Note: Given the stochastic nature of the algorithm, your specific results may vary. Consider running the example a few times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238c92b",
   "metadata": {},
   "source": [
    "## 25.6 Extensions\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "- Data Preparation. Consider exploring whether simple data scaling schemes can further lift model performance, such as normalization, standardization, and power transforms.\n",
    "- LSTM Variations. There are variations of the LSTM architecture that may achieve better performance on this problem, such as stacked LSTMs and Bidirectional LSTMs.\n",
    "- Hyperparameter Tuning. Consider exploring tuning of model hyperparameters such as the number of units, training epochs, batch size, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4da44",
   "metadata": {},
   "source": [
    "## 25.8 Summary\n",
    "In this tutorial, you discovered three recurrent neural network architectures for modeling an activity recognition time series classification problem. Specifically, you learned:\n",
    "- How to develop a Long Short-Term Memory Recurrent Neural Network for human activity recognition.\n",
    "- How to develop a one-dimensional Convolutional Neural Network LSTM, or CNN-LSTM,\n",
    "model.\n",
    "- How to develop a one-dimensional Convolutional LSTM, or ConvLSTM, model for the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1961d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy: 1.6.2\n",
      "numpy: 1.21.5\n",
      "matplotlib: 3.3.4\n",
      "pandas: 1.2.4\n",
      "statsmodels: 0.12.2\n",
      "sklearn: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "# check library version numbers\n",
    "# scipy\n",
    "import scipy\n",
    "print('scipy: %s' % scipy.__version__) # numpy\n",
    "import numpy\n",
    "print('numpy: %s' % numpy.__version__)\n",
    "# matplotlib\n",
    "import matplotlib\n",
    "print('matplotlib: %s' % matplotlib.__version__)\n",
    "# pandas\n",
    "import pandas\n",
    "print('pandas: %s' % pandas.__version__)\n",
    "# statsmodels\n",
    "import statsmodels\n",
    "print('statsmodels: %s' % statsmodels.__version__) # scikit-learn\n",
    "import sklearn\n",
    "print('sklearn: %s' % sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "python versions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf926853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# theano\n",
    "import theano\n",
    "print('theano: %s' % theano.__version__)\n",
    "# tensorflow\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__) # keras\n",
    "import keras\n",
    "print('keras: %s' % keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d8c521",
   "metadata": {},
   "source": [
    "How Far You Have Come\n",
    "You made it. Well done. Take a moment and look back at how far you have come. You now know:\n",
    "- About the promise of neural networks and deep learning methods in general for time series forecasting.\n",
    "- How to transform time series data in order to train a supervised learning algorithm, such as deep learning methods.\n",
    "- How to develop baseline forecasts using naive and classical methods by which to determine whether forecasts from deep learning models have skill or not.\n",
    "- How to develop Multilayer Perceptron, Convolutional Neural Network, Long Short-Term Memory Networks, and hybrid neural network models for time series forecasting.\n",
    "- How to forecast univariate, multivariate, multi-step, and multivariate multi-step time series forecasting problems in general.\n",
    "- How to transform sequence data into a three-dimensional structure in order to train convolutional and LSTM neural network models.\n",
    "- How to grid search deep learning model hyperparameters to ensure that you are getting good performance from a given model.\n",
    "- How to prepare data and develop deep learning models for forecasting a range of univariate time series problems with different temporal structures.\n",
    "- How to prepare data and develop deep learning models for multi-step forecasting a real-world household electricity consumption dataset.\n",
    "- How to prepare data and develop deep learning models for a real-world human activity recognition project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e87928",
   "metadata": {},
   "source": [
    "Don’t make light of this. You have come a long way in a short amount of time. You have developed the important and valuable set of skills in developing deep learning models for time series forecasting. You can now confidently:\n",
    "- Use naive and classical methods like SARIMA and ETS to quickly develop robust baseline models for a range of different time series forecasting problems, the performance of which can be used to challenge whether more elaborate machine learning and deep learning models are adding value.\n",
    "- Transform native time series forecasting data into a form for fitting supervised learning algorithms and confidently tune the amount of lag observations and framing of the prediction problem.\n",
    "- Develop MLP, CNN, RNN, and hybrid deep learning models quickly for a range of different time series forecasting problems, and confidently evaluate and interpret their performance.\n",
    "\n",
    "The sky’s the limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79218a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
