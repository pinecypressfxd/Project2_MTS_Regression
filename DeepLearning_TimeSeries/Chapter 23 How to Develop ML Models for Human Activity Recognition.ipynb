{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4990186",
   "metadata": {},
   "source": [
    "# Chapter 23 How to Develop ML Models for Human Activity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7fab83",
   "metadata": {},
   "source": [
    "In this tutorial, you will discover how to evaluate a diverse suite of machine learning algorithms on the Activity Recognition Using Smartphones dataset. After completing this tutorial, you will know:\n",
    "- How to load and evaluate nonlinear and ensemble machine learning algorithms on the feature-engineered version of the activity recognition dataset.\n",
    "- How to load and evaluate machine learning algorithms on the raw signal data for the activity recognition dataset.\n",
    "- How to define reasonable lower and upper bounds on the expected performance of more sophisticated algorithms capable of feature learning, such as deep learning methods.\n",
    "Let’s get started.\n",
    "\n",
    "## 23.1 Tutorial Overview\n",
    "This tutorial is divided into three parts; they are:\n",
    "1. Activity Recognition Using Smartphones Dataset \n",
    "2. Modeling Feature Engineered Data\n",
    "3. Modeling Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54be61",
   "metadata": {},
   "source": [
    "## 23.2 Activity Recognition Using Smartphones Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbeb45e",
   "metadata": {},
   "source": [
    "see Chapter 22."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bec4e0",
   "metadata": {},
   "source": [
    "## 23.3 Modeling Feature Engineered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a9bf1",
   "metadata": {},
   "source": [
    "In this section, we will develop code to load the feature-engineered version of the dataset and evaluate a suite of nonlinear machine learning algorithms, including SVM used in the original paper. The goal is to achieve at least 89% accuracy on the test dataset.\n",
    "\n",
    "1. Load Dataset\n",
    "2. Define Models\n",
    "3. Evaluate Models\n",
    "4. Summarize Results \n",
    "5. Complete Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71172bf",
   "metadata": {},
   "source": [
    "### 23.3.1 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5441c8ef",
   "metadata": {},
   "source": [
    "- HARDataset/train/X_train.txt \n",
    "- HARDataset/train/y_train.txt \n",
    "- HARDataset/test/X_test.txt\n",
    "- HARDataset/test/y_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a6e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (7352, 561) (7352, 1)\n",
      "test:  (2947, 561) (2947, 1)\n",
      "flatten y: (7352, 561) (7352,) (2947, 561) (2947,)\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "# load a single file as a numpy array\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    # load input data\n",
    "    X = load_file(prefix + group + '/X_'+group+'.txt') \n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    print(\"train:\",trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/') \n",
    "    print(\"test: \",testX.shape, testy.shape)\n",
    "    # flatten y\n",
    "    trainy, testy = trainy[:,0], testy[:,0]\n",
    "    print('flatten y:', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "# load dataset\n",
    "trainX, trainy, testX, testy = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce7a92",
   "metadata": {},
   "source": [
    "### 23.3.2 Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f6014",
   "metadata": {},
   "source": [
    "We will evaluate the models using default configurations. We are not looking for optimal configurations of these models at this point, just a general idea of how well sophisticated models with default configurations perform on this problem. We will evaluate a diverse set of nonlinear and ensemble machine learning algorithms, specifically:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e5e23",
   "metadata": {},
   "source": [
    "Nonlinear Algorithms:\n",
    "- k-Nearest Neighbors\n",
    "- Classification and Regression Tree \n",
    "- Support Vector Machine\n",
    "- Naive Bayes\n",
    "\n",
    "Ensemble Algorithms:\n",
    "- Bagged Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "- Gradient Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0985b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "    models['knn'] = KNeighborsClassifier(n_neighbors=7) \n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "    models['bag'] = BaggingClassifier(n_estimators=100) \n",
    "    models['rf'] = RandomForestClassifier(n_estimators=100) \n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=100) \n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=100) \n",
    "    print('Defined %d models' % len(models))\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18eda69",
   "metadata": {},
   "source": [
    "### 23.3.3 Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871c131",
   "metadata": {},
   "source": [
    "In this case we will use classification accuracy that will capture the performance (or error) of a model given the balance observations across the six activities (or classes). \n",
    "\n",
    "The evaluate model() function below implements this behavior, evaluating a given model and returning **the classification accuracy** as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e5996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(testy, yhat)\n",
    "    return accuracy * 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7d31e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name])) \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135cf4ee",
   "metadata": {},
   "source": [
    "### 23.3.4 Summarize Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0ef90",
   "metadata": {},
   "source": [
    "The final step is to summarize the findings. We can sort all of the results by the classification accuracy in descending order because we are interested in maximizing accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d74fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores: \n",
    "        print('Name=%s, Score=%.3f' % (name, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45749495",
   "metadata": {},
   "source": [
    "### 23.3.5 Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2423fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (7352, 561) (7352, 1)\n",
      "test:  (2947, 561) (2947, 1)\n",
      "flatten y: (7352, 561) (7352,) (2947, 561) (2947,)\n",
      "Defined 8 models\n",
      ">knn: 90.329\n",
      ">cart: 85.409\n",
      ">svm: 95.046\n",
      ">bayes: 77.027\n",
      ">bag: 89.922\n",
      ">rf: 92.535\n",
      ">et: 93.824\n",
      ">gbm: 93.892\n",
      "\n",
      "Name=svm, Score=95.046\n",
      "Name=gbm, Score=93.892\n",
      "Name=et, Score=93.824\n",
      "Name=rf, Score=92.535\n",
      "Name=knn, Score=90.329\n",
      "Name=bag, Score=89.922\n",
      "Name=cart, Score=85.409\n",
      "Name=bayes, Score=77.027\n"
     ]
    }
   ],
   "source": [
    "# spot check ml algorithms on engineered-features from the har dataset\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    # load input data\n",
    "    X = load_file(prefix + group + '/X_'+group+'.txt') \n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    print(\"train:\",trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/') \n",
    "    print(\"test: \",testX.shape, testy.shape)\n",
    "    # flatten y\n",
    "    trainy, testy = trainy[:,0], testy[:,0]\n",
    "    print('flatten y:', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "    models['knn'] = KNeighborsClassifier(n_neighbors=7) \n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "    models['bag'] = BaggingClassifier(n_estimators=100) \n",
    "    models['rf'] = RandomForestClassifier(n_estimators=100) \n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=100) \n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=100) \n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(testy, yhat)\n",
    "    return accuracy * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name])) \n",
    "    return results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores: \n",
    "        print('Name=%s, Score=%.3f' % (name, score))\n",
    "        \n",
    "if __name__ == '__main__':        \n",
    "    # load dataset\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # get model list\n",
    "    models = define_models()\n",
    "    # evaluate models\n",
    "    results = evaluate_models(trainX, trainy, testX, testy, models)\n",
    "    # summarize results\n",
    "    summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9950a9a",
   "metadata": {},
   "source": [
    "Running the example first loads the train and test datasets. The eight models are then evaluated in turn, printing the performance for each. Finally, a rank of the models by their performance on the test set is displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fca2a0",
   "metadata": {},
   "source": [
    "We can see that both the ExtraTrees ensemble method and the Support Vector Machines nonlinear methods achieve a performance of about 94% accuracy on the test set. This is a great result, exceeding the reported 89% by SVM in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5b867",
   "metadata": {},
   "source": [
    "## 23.4 Modeling Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b421ebeb",
   "metadata": {},
   "source": [
    "The raw data does require some more work to load. There are three main signal types in the raw data: **total acceleration**, **body acceleration**, and **body gyroscope**. Each has three axes of data. This means that there are a total of **nine variables** for each time step. \n",
    "\n",
    "Further, each series of data has been partitioned into overlapping windows of **2.65** seconds of data, or **128 time steps**. These windows of data correspond to the windows of engineered features (rows) in the previous section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcff387",
   "metadata": {},
   "source": [
    "This means that one row of data has 128 × 9 or 1,152 elements. This is a little less than double the size of the 561 element vectors in the previous section and it is likely that there is some redundant data. The signals are stored in the /Inertial Signals/ directory under the train and test subdirectories. Each axis of each signal is stored in a separate file, meaning that each of the train and test datasets have nine input files to load and one output file to load. We can batch the loading of these files into groups given the consistent directory structures and file naming conventions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44174c98",
   "metadata": {},
   "source": [
    "First, we can load all data for a given group into a single three-dimensional NumPy array, where the dimensions of the array are [samples, timesteps, features]. To make this clearer, there are 128 time steps and nine features, where the number of samples is the number of rows in any given raw signal data file. The load group() function below implements this behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e104e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb975f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/') \n",
    "    print(testX.shape, testy.shape)\n",
    "    # flatten X\n",
    "    trainX = trainX.reshape((trainX.shape[0], trainX.shape[1] * trainX.shape[2]))\n",
    "    testX = testX.reshape((testX.shape[0], testX.shape[1] * testX.shape[2]))\n",
    "    # flatten y\n",
    "    trainy, testy = trainy[:,0], testy[:,0]\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2da77",
   "metadata": {},
   "source": [
    "**Putting this all together, the complete example is listed below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47618ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 1152) (7352,) (2947, 1152) (2947,)\n",
      "Defined 8 models\n",
      ">knn: 61.893\n",
      ">cart: 71.802\n",
      ">svm: 88.734\n",
      ">bayes: 72.480\n",
      ">bag: 84.934\n",
      ">rf: 84.628\n",
      ">et: 86.495\n",
      ">gbm: 87.207\n",
      "\n",
      "Name=svm, Score=88.734\n",
      "Name=gbm, Score=87.207\n",
      "Name=et, Score=86.495\n",
      "Name=bag, Score=84.934\n",
      "Name=rf, Score=84.628\n",
      "Name=bayes, Score=72.480\n",
      "Name=cart, Score=71.802\n",
      "Name=knn, Score=61.893\n"
     ]
    }
   ],
   "source": [
    "# spot check on raw data from the har dataset\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def load_file(filepath):\n",
    "    dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
    "    return dataframe.values\n",
    "\n",
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''): \n",
    "    loaded = list()\n",
    "    for name in filenames:\n",
    "        data = load_file(prefix + name)\n",
    "        loaded.append(data)\n",
    "    # stack group so that features are the 3rd dimension\n",
    "    loaded = dstack(loaded)\n",
    "    return loaded\n",
    "\n",
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "    filepath = prefix + group + '/Inertial Signals/'\n",
    "    # load all 9 files as a single array\n",
    "    filenames = list()\n",
    "    # total acceleration\n",
    "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt',\n",
    "    'total_acc_z_'+group+'.txt']\n",
    "    # body acceleration\n",
    "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt',\n",
    "    'body_acc_z_'+group+'.txt']\n",
    "    # body gyroscope\n",
    "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt',\n",
    "    'body_gyro_z_'+group+'.txt']\n",
    "    # load input data\n",
    "    X = load_group(filenames, filepath)\n",
    "    # load class output\n",
    "    y = load_file(prefix + group + '/y_'+group+'.txt') \n",
    "    return X, y\n",
    "\n",
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "    # load all train\n",
    "    trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/') \n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load all test\n",
    "    testX, testy = load_dataset_group('test', prefix + 'HARDataset/') \n",
    "    print(testX.shape, testy.shape)\n",
    "    # flatten X\n",
    "    trainX = trainX.reshape((trainX.shape[0], trainX.shape[1] * trainX.shape[2]))\n",
    "    testX = testX.reshape((testX.shape[0], testX.shape[1] * testX.shape[2]))\n",
    "    # flatten y\n",
    "    trainy, testy = trainy[:,0], testy[:,0]\n",
    "    print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # nonlinear models\n",
    "    models['knn'] = KNeighborsClassifier(n_neighbors=7) \n",
    "    models['cart'] = DecisionTreeClassifier()\n",
    "    models['svm'] = SVC()\n",
    "    models['bayes'] = GaussianNB()\n",
    "    # ensemble models\n",
    "    models['bag'] = BaggingClassifier(n_estimators=100) \n",
    "    models['rf'] = RandomForestClassifier(n_estimators=100) \n",
    "    models['et'] = ExtraTreesClassifier(n_estimators=100) \n",
    "    models['gbm'] = GradientBoostingClassifier(n_estimators=100) \n",
    "    print('Defined %d models' % len(models))\n",
    "    return models\n",
    "\n",
    "# evaluate a single model\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "    # fit the model\n",
    "    model.fit(trainX, trainy)\n",
    "    # make predictions\n",
    "    yhat = model.predict(testX)\n",
    "    # evaluate predictions\n",
    "    accuracy = accuracy_score(testy, yhat)\n",
    "    return accuracy * 100.0\n",
    "\n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(trainX, trainy, testX, testy, models):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        results[name] = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "        # show process\n",
    "        print('>%s: %.3f' % (name, results[name])) \n",
    "    return results\n",
    "\n",
    "# print and plot the results\n",
    "def summarize_results(results, maximize=True):\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,v) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    print()\n",
    "    for name, score in mean_scores: \n",
    "        print('Name=%s, Score=%.3f' % (name, score))\n",
    "        \n",
    "if __name__ == '__main__':        \n",
    "    # load dataset\n",
    "    trainX, trainy, testX, testy = load_dataset()\n",
    "    # get model list\n",
    "    models = define_models()\n",
    "    # evaluate models\n",
    "    results = evaluate_models(trainX, trainy, testX, testy, models)\n",
    "    # summarize results\n",
    "    summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef587436",
   "metadata": {},
   "source": [
    "As noted in the previous section, these results provide a lower-bound on accuracy for any more sophisticated methods that may attempt to learn higher order features automatically (e.g. via feature learning in deep learning methods) from the raw data. In summary, the bounds for such methods extend on this dataset from about 87% accuracy with GBM on the raw data to about 94% with Extra Trees and SVM on the highly processed dataset, [87% to 94%]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2eb5da",
   "metadata": {},
   "source": [
    "如上一节所述，这些结果为任何更复杂的方法提供了精度的下限，这些方法可能会尝试从原始数据中自动学习高阶特征（例如，通过深度学习方法中的特征学习）。总之，这些方法的边界在这个数据集上从原始数据上使用GBM的约87%的准确率扩展到高度处理的数据集上使用额外树和SVM的约94%，[87%到94%]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54072ce",
   "metadata": {},
   "source": [
    "## 23.5 Extensions\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "- More Algorithms. Only eight machine learning algorithms were evaluated on the problem; try some linear methods and perhaps some more nonlinear and ensemble methods.\n",
    "- Algorithm Tuning. No tuning of the machine learning algorithms was performed; mostly default configurations were used. Pick a method such as SVM, ExtraTrees, or Gradient Boosting and grid search a suite of different hyperparameter configurations to see if you can further lift performance on the problem.\n",
    "- Data Scaling. The data is already scaled to [-1,1], perhaps per subject. Explore whether additional scaling, such as standardization, can result in better performance, perhaps on methods sensitive to such scaling such as kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d216e32",
   "metadata": {},
   "source": [
    "## 23.7 Summary\n",
    "In this tutorial, you discovered how to evaluate a diverse suite of machine learning algorithms on the Activity Recognition Using Smartphones dataset. Specifically, you learned:\n",
    "- How to load and evaluate nonlinear and ensemble machine learning algorithms on the feature-engineered version of the activity recognition dataset.\n",
    "- How to load and evaluate machine learning algorithms on the raw signal data for the activity recognition dataset.\n",
    "- How to define reasonable lower and upper bounds on the expected performance of more sophisticated algorithms capable of feature learning, such as deep learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40623920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
